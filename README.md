Vision Transformer (ViT) is a type of model architecture that applies Transformer mechanisms—originally designed for natural language processing—to visual data. 
Unlike traditional convolutional neural networks (CNNs), which work directly with pixel values, ViT treats image patches as tokens similar to words in NLP tasks.

![vit_figure](https://github.com/user-attachments/assets/a06b34ed-55e5-4e9f-abac-4191e3df8bca)
